{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION USING LSTM AND CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification I am using OLID datset. More information and the dataset can be found here: https://sites.google.com/site/offensevalsharedtask/offenseval2019\n",
    "\n",
    "First task is to classify whether a given tweet is offensive or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see sample of the training data. For this task we are only interested in tweet and subtask_a which is the label containing either OFF (OFFENSIVE) or NOT.   The training dataset contains tweets and labels in a single tsv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('OLIDv1.0/olid-training-v1.0.tsv',sep='\\t')\n",
    "df = pd.DataFrame(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = [tweet.split() for tweet in df['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['@USER', 'She', 'should', 'ask', 'a', 'few', 'native', 'Americans', 'what', 'their', 'take', 'on', 'this', 'is.'], ['@USER', '@USER', 'Go', 'home', 'youâ€™re', 'drunk!!!', '@USER', '#MAGA', '#Trump2020', 'ðŸ‘ŠðŸ‡ºðŸ‡¸ðŸ‘Š', 'URL'], ['Amazon', 'is', 'investigating', 'Chinese', 'employees', 'who', 'are', 'selling', 'internal', 'data', 'to', 'third-party', 'sellers', 'looking', 'for', 'an', 'edge', 'in', 'the', 'competitive', 'marketplace.', 'URL', '#Amazon', '#MAGA', '#KAG', '#CHINA', '#TCOT'], ['@USER', 'Someone', 'should\\'veTaken\"', 'this', 'piece', 'of', 'shit', 'to', 'a', 'volcano.', 'ðŸ˜‚\"'], ['@USER', '@USER', 'Obama', 'wanted', 'liberals', '&amp;', 'illegals', 'to', 'move', 'into', 'red', 'states']]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_list[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing needs to be performed on the tweets. The preprocessing performed is as follows:\n",
    "1. Convert all words to lower case\n",
    "2. remove all non alphanumeric characters (#, : , emojis)\n",
    "3. remove words such as 'user' and 'url' as it does not give any useful information\n",
    "4. lemmatize words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess(tweets_list):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    clean_tweet = []\n",
    "    for tweet in tweets_list:\n",
    "        new_tweet = []\n",
    "        for word in tweet:\n",
    "            word = word.lower()\n",
    "            word = re.sub('[\\W_]', '', word)\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            if word !='' and word!=\"user\" and word!=\"url\":\n",
    "                new_tweet.append(word)\n",
    "        clean_tweet.append(new_tweet)\n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras Tokenizer is used which converts sentences to integers which can then be fed into neural network. WE also need to pad sequences so all of them have same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "clean_tweet = preprocess(tweets_list)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(clean_tweet)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "sents_as_ids = tokenizer.texts_to_sequences(clean_tweet)\n",
    "VOCAB_SIZE = len(word2idx)+1  # 0 saved for padding so we add 1\n",
    "MAXIMUM_LENGTH = 30\n",
    "processed_train_data = pad_sequences(sents_as_ids,MAXIMUM_LENGTH,padding ='post',truncating='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert labels to integers (OFF=1, NOT=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for label in df['subtask_a']:\n",
    "    if label=='OFF':\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perform the same steps on test data. Here the tweet and labels are stored in two different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99, 156, 29, 631, 5200, 2688, 2281, 6756, 241, 8448, 1117, 1552, 687, 324, 817, 5, 1424, 2382, 375, 354]\n"
     ]
    }
   ],
   "source": [
    "test_set = pd.read_csv('OLIDv1.0/testset-levela.tsv',sep='\\t')\n",
    "test_labels = pd.read_csv('OLIDv1.0/labels-levela.csv',header=None)\n",
    "\n",
    "test_set_df = pd.DataFrame(test_set)\n",
    "test_labels_df = pd.DataFrame(test_labels)\n",
    "test_tweets_list = [tweet.split() for tweet in test_set_df['tweet']]\n",
    "test_clean_tweets = preprocess(test_tweets_list)\n",
    "test_sents_as_ids = tokenizer.texts_to_sequences(test_clean_tweets)\n",
    "processed_test_data = pad_sequences(test_sents_as_ids,MAXIMUM_LENGTH,padding ='post',truncating='post')\n",
    "\n",
    "test_labels = []\n",
    "for label in test_labels_df.iloc[:,1]:\n",
    "    if label == 'OFF':\n",
    "        test_labels.append(1)\n",
    "    else:\n",
    "        test_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, None, 10)          188720    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, None, 10)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 200,971\n",
      "Trainable params: 200,971\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#HYPERPARAMETERS\n",
    "EMBD_SIZE =10\n",
    "LSTM_dropout_rate = 0.3\n",
    "HIDDEN_UNITS = 50\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(VOCAB_SIZE,EMBD_SIZE))\n",
    "model.add(keras.layers.Dropout(rate=LSTM_dropout_rate))\n",
    "model.add(keras.layers.LSTM(units=HIDDEN_UNITS))\n",
    "model.add(keras.layers.Dropout(LSTM_dropout_rate))\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = processed_train_data[:10000]\n",
    "train_labels = labels[:10000]\n",
    "val_set = processed_train_data[10000:]\n",
    "val_labels = labels[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 3240 samples\n",
      "Epoch 1/4\n",
      "10000/10000 [==============================] - 5s 455us/step - loss: 0.1529 - accuracy: 0.9448 - val_loss: 0.9183 - val_accuracy: 0.7154\n",
      "Epoch 2/4\n",
      "10000/10000 [==============================] - 5s 466us/step - loss: 0.1323 - accuracy: 0.9545 - val_loss: 0.8051 - val_accuracy: 0.7377\n",
      "Epoch 3/4\n",
      "10000/10000 [==============================] - 5s 457us/step - loss: 0.1247 - accuracy: 0.9587 - val_loss: 0.8290 - val_accuracy: 0.7157\n",
      "Epoch 4/4\n",
      "10000/10000 [==============================] - 5s 457us/step - loss: 0.1186 - accuracy: 0.9611 - val_loss: 0.9748 - val_accuracy: 0.7228\n",
      "RESULTS ON TEST DATA\n",
      "860/860 [==============================] - 0s 116us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.984401518522307, 0.739534854888916]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "EPOCHS = 4\n",
    "\n",
    "history = model.fit(train_data,train_labels,batch_size=BATCH_SIZE,epochs=EPOCHS,verbose=1,validation_data=(val_set,val_labels))\n",
    "print(\"RESULTS ON TEST DATA\")\n",
    "model.evaluate(processed_test_data,test_labels,batch_size=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model is based on CNN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, None, 10)          188720    \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, None, 32)          992       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_24  (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 189,745\n",
      "Trainable params: 189,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#HYPERPARAMETERS\n",
    "CNN_EMBD_SIZE = 10\n",
    "num_of_filters = 32\n",
    "filter_size = 3\n",
    "dropout_rate = 0.2\n",
    "\n",
    "cnn_model = keras.Sequential()\n",
    "cnn_model.add(keras.layers.Embedding(VOCAB_SIZE,CNN_EMBD_SIZE))\n",
    "cnn_model.add(keras.layers.Conv1D(num_of_filters,filter_size,activation='relu'))\n",
    "cnn_model.add(keras.layers.GlobalAveragePooling1D())\n",
    "cnn_model.add(keras.layers.Dropout(dropout_rate))\n",
    "cnn_model.add(keras.layers.Dense(1,activation='sigmoid'))\n",
    "cnn_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 3240 samples\n",
      "Epoch 1/3\n",
      "10000/10000 [==============================] - 1s 81us/step - loss: 0.3440 - accuracy: 0.8599 - val_loss: 0.5329 - val_accuracy: 0.7685\n",
      "Epoch 2/3\n",
      "10000/10000 [==============================] - 1s 79us/step - loss: 0.2834 - accuracy: 0.8874 - val_loss: 0.5642 - val_accuracy: 0.7633\n",
      "Epoch 3/3\n",
      "10000/10000 [==============================] - 1s 82us/step - loss: 0.2369 - accuracy: 0.9120 - val_loss: 0.6076 - val_accuracy: 0.7642\n",
      "860/860 [==============================] - 0s 41us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5626795350812203, 0.7930232286453247]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = cnn_model.fit(train_data,train_labels,batch_size=50,\n",
    "                    epochs=3,verbose=1,validation_data=(val_set,val_labels))\n",
    "\n",
    "cnn_model.evaluate(processed_test_data,test_labels,batch_size=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
